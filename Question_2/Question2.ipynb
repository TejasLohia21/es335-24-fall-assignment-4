{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import time\n",
    "import sys\n",
    "import tracemalloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(a,b):\n",
    "    return np.sqrt(np.sum((a - b)**2))                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(N, D):\n",
    "    return np.random.rand(N, D)\n",
    "\n",
    "def generate_query(D):\n",
    "    return np.random.rand(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_knn(query, dataset, K):\n",
    "    distances = [distance(query, point) for point in dataset]\n",
    "    sorted_indices = np.argsort(distances)\n",
    "\n",
    "    k_sorted_points =  np.array([dataset[i] for i in sorted_indices[:K]])\n",
    "    k_sorted_distances = np.array(distances)[sorted_indices[:K]]\n",
    "\n",
    "    return sorted_indices[:K], k_sorted_points,  k_sorted_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N,D,K= 1000, 5, 10\n",
    "ds= generate_dataset(N,D)\n",
    "q= generate_query(D)\n",
    "if D==2:\n",
    "    plt.scatter(ds[:, 0], ds[:, 1])\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.title('Scatter plot of 2D Dataset')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n",
      "[371  59 636 632  35 864 720 266 681 958]\n",
      "[[0.86951383 0.09239464 0.38818843 0.91664639 0.92468818]\n",
      " [0.85552405 0.01579039 0.33846085 0.83898189 0.61377044]\n",
      " [0.76167036 0.2609945  0.21365706 0.93584771 0.73211638]\n",
      " [0.77120675 0.06450603 0.40366192 0.80754863 0.56411609]\n",
      " [0.9413087  0.08627977 0.07070335 0.99889504 0.8461975 ]\n",
      " [0.93387277 0.02893653 0.10424307 0.91198749 0.60313695]\n",
      " [0.62704784 0.04864034 0.49964467 0.92428542 0.83798771]\n",
      " [0.74082376 0.16473439 0.18874916 0.78536955 0.55504274]\n",
      " [0.61781287 0.04742206 0.09300555 0.93059198 0.74878784]\n",
      " [0.97369146 0.0387304  0.46513792 0.7260619  0.52914708]]\n",
      "[0.14953606 0.19931195 0.26879001 0.28338219 0.28645312 0.30006675\n",
      " 0.31720142 0.34201022 0.35632783 0.35709567]\n"
     ]
    }
   ],
   "source": [
    "indices, points, distances=naive_knn(q, ds, K)\n",
    "print(points.shape)\n",
    "print(indices)\n",
    "print(points)\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KD Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDTree:\n",
    "    def __init__(self, data, leaf_size):\n",
    "        self.data = np.hstack((data, np.arange(len(data)).reshape(-1, 1))) ##stacking the indices\n",
    "        self.leaf_size = leaf_size\n",
    "        self.tree = self.build_kdtree(self.data)\n",
    "    \n",
    "    def build_kdtree(self, data, depth=0):\n",
    "        if len(data) <= self.leaf_size:\n",
    "            return data  \n",
    "        \n",
    "        axis = depth % (data.shape[1] - 1)  # Alternate splitting axis, ignore index column\n",
    "        sorted_data = data[data[:, axis].argsort()]\n",
    "        median_index = len(sorted_data) // 2\n",
    "        left = self.build_kdtree(sorted_data[:median_index], depth + 1)\n",
    "        right = self.build_kdtree(sorted_data[median_index + 1:], depth + 1)\n",
    "        \n",
    "        return (sorted_data[median_index], left, right)\n",
    "    \n",
    "    def query(self, query, K):\n",
    "        indices, distances = self._query(self.tree, query, K, depth=0) \n",
    "        points = self.data[indices, :-1]\n",
    "        return indices, points, distances\n",
    "    \n",
    "    def _query(self, node, query, K, depth):\n",
    "    \n",
    "        if isinstance(node, np.ndarray): #leaf node is an array\n",
    "            points = node[:, :-1] \n",
    "            original_indices = node[:, -1].astype(int)  \n",
    "            distances = np.array([distance(query, point) for point in points])\n",
    "            sorted_indices = np.argsort(distances)\n",
    "            nearest_indices = original_indices[sorted_indices[:K]]\n",
    "            nearest_distances = distances[sorted_indices[:K]]\n",
    "            return nearest_indices, nearest_distances\n",
    "        \n",
    "        if isinstance(node, tuple) and len(node) == 3: # non-leaf node is a tuple (median point, left subtree, right subtree)\n",
    "            median, left, right = node\n",
    "            axis = depth % (query.shape[0])\n",
    "            \n",
    "            if query[axis] < median[axis]:\n",
    "                primary, other = left, right\n",
    "            else:\n",
    "                primary, other = right, left\n",
    "            \n",
    "            # Recursively search the primary side\n",
    "            indices, distances = self._query(primary, query, K, depth + 1)\n",
    "            \n",
    "            \n",
    "            if len(indices) < K or abs(query[axis] - median[axis]) < max(distances): # if the other tree's data also need to be combined\n",
    "                other_indices, other_distances = self._query(other, query, K, depth + 1)\n",
    "                combined_indices = np.concatenate([indices, other_indices])\n",
    "                combined_distances = np.concatenate([distances, other_distances])\n",
    "                sorted_combined = np.argsort(combined_distances)\n",
    "                indices = combined_indices[sorted_combined][:K]\n",
    "                distances = combined_distances[sorted_combined][:K]\n",
    "            \n",
    "            return indices, distances\n",
    "        \n",
    "       \n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of nearest neighbors: [371  59 632  35 864 720 266 681 958 226]\n",
      "Nearest points: [[0.86951383 0.09239464 0.38818843 0.91664639 0.92468818]\n",
      " [0.85552405 0.01579039 0.33846085 0.83898189 0.61377044]\n",
      " [0.77120675 0.06450603 0.40366192 0.80754863 0.56411609]\n",
      " [0.9413087  0.08627977 0.07070335 0.99889504 0.8461975 ]\n",
      " [0.93387277 0.02893653 0.10424307 0.91198749 0.60313695]\n",
      " [0.62704784 0.04864034 0.49964467 0.92428542 0.83798771]\n",
      " [0.74082376 0.16473439 0.18874916 0.78536955 0.55504274]\n",
      " [0.61781287 0.04742206 0.09300555 0.93059198 0.74878784]\n",
      " [0.97369146 0.0387304  0.46513792 0.7260619  0.52914708]\n",
      " [0.90776489 0.08991513 0.03769023 0.67790346 0.90707521]]\n",
      "Distances to nearest points: [0.14953606 0.19931195 0.28338219 0.28645312 0.30006675 0.31720142\n",
      " 0.34201022 0.35632783 0.35709567 0.36787607]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tree = KDTree(ds, leaf_size=20) ## training\n",
    "\n",
    "\n",
    "indices, points, distances = tree.query(q, K) ##testing\n",
    "\n",
    "print(\"Indices of nearest neighbors:\", indices)\n",
    "print(\"Nearest points:\", points)\n",
    "print(\"Distances to nearest points:\", distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.011142492294311523 seconds\n",
      "Memory usage after training: 56 bytes\n",
      "Testing time: 0.0049855709075927734 seconds\n",
      "Indices of nearest neighbors: [371  59 632  35 864 720 266 681 958 226]\n",
      "Nearest points: [[0.86951383 0.09239464 0.38818843 0.91664639 0.92468818]\n",
      " [0.85552405 0.01579039 0.33846085 0.83898189 0.61377044]\n",
      " [0.77120675 0.06450603 0.40366192 0.80754863 0.56411609]\n",
      " [0.9413087  0.08627977 0.07070335 0.99889504 0.8461975 ]\n",
      " [0.93387277 0.02893653 0.10424307 0.91198749 0.60313695]\n",
      " [0.62704784 0.04864034 0.49964467 0.92428542 0.83798771]\n",
      " [0.74082376 0.16473439 0.18874916 0.78536955 0.55504274]\n",
      " [0.61781287 0.04742206 0.09300555 0.93059198 0.74878784]\n",
      " [0.97369146 0.0387304  0.46513792 0.7260619  0.52914708]\n",
      " [0.90776489 0.08991513 0.03769023 0.67790346 0.90707521]]\n",
      "Distances to nearest points: [0.14953606 0.19931195 0.28338219 0.28645312 0.30006675 0.31720142\n",
      " 0.34201022 0.35632783 0.35709567 0.36787607]\n"
     ]
    }
   ],
   "source": [
    "# ______________________________Remove afer seeing__________________________________\n",
    "\n",
    "\n",
    "# Measure training time and memory\n",
    "start_train_time = time.time()\n",
    "tree = KDTree(ds, leaf_size=20)\n",
    "end_train_time = time.time()\n",
    "training_time = end_train_time - start_train_time\n",
    "training_memory = sys.getsizeof(tree)\n",
    "\n",
    "print(\"Training time:\", training_time, \"seconds\")\n",
    "print(\"Memory usage after training:\", training_memory, \"bytes\")\n",
    "\n",
    "# Measure testing time and memory\n",
    "start_test_time = time.time()\n",
    "indices, points, distances = tree.query(q, K)\n",
    "end_test_time = time.time()\n",
    "testing_time = end_test_time - start_test_time\n",
    "\n",
    "print(\"Testing time:\", testing_time, \"seconds\")\n",
    "print(\"Indices of nearest neighbors:\", indices)\n",
    "print(\"Nearest points:\", points)\n",
    "print(\"Distances to nearest points:\", distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH \n",
    "with flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSH:\n",
    "#     def __init__(self, dataset, num_hashes=5):\n",
    "#         self.dataset = dataset\n",
    "#         self.num_hashes = num_hashes\n",
    "#         self.dataset_augmented = np.hstack((self.dataset, np.ones((self.dataset.shape[0], 1))))  # Adding bias term\n",
    "#         self.hashes = self._generate_hashes()  # Hashes with bias term included\n",
    "\n",
    "#     def _generate_hashes(self):\n",
    "#         return np.random.randn(self.num_hashes, self.dataset.shape[1] + 1)\n",
    "\n",
    "#     def _hash_(self, point):\n",
    "#         point_augmented = np.append(point, 1)  \n",
    "#         return np.sign(np.dot(self.hashes, point_augmented))  \n",
    "\n",
    "#     def query(self, query, K):\n",
    "#         query_hash = self._hash_(query)\n",
    "\n",
    "#         hash_buckets = {}\n",
    "#         for index, point in enumerate(self.dataset):\n",
    "#             point_hash = tuple(self._hash_(point))  # Made it into a tuple so it can be used as a dictionary key\n",
    "#             if point_hash not in hash_buckets:\n",
    "#                 hash_buckets[point_hash] = []\n",
    "#             hash_buckets[point_hash].append(index)\n",
    "\n",
    "#         query_hash_tuple = tuple(query_hash)\n",
    "\n",
    "#         nearest_neighbors = self.find_k(query, hash_buckets.get(query_hash_tuple, []), K) # looking into same bucket\n",
    "\n",
    "#         if len(nearest_neighbors) < K: #neighbouring buckets\n",
    "#             for i in range(self.num_hashes):\n",
    "#                 if len(nearest_neighbors) >= K:\n",
    "#                     break\n",
    " \n",
    "#                 modified_query_hash = list(query_hash_tuple)\n",
    "#                 modified_query_hash[i] = 1 - modified_query_hash[i]  # Flipping the ith hash value\n",
    "#                 modified_query_hash_tuple = tuple(modified_query_hash)\n",
    "                \n",
    "#                 if modified_query_hash_tuple in hash_buckets:\n",
    "#                     remaining_neighbors = self.find_k(query, hash_buckets[modified_query_hash_tuple], K - len(nearest_neighbors))\n",
    "#                     nearest_neighbors.extend(remaining_neighbors)\n",
    "\n",
    "#         nearest_neighbors = sorted(nearest_neighbors, key=lambda x: x[1])[:K] \n",
    "\n",
    "#         nearest_indices = [index for index, _ in nearest_neighbors]\n",
    "#         nearest_points = self.dataset[nearest_indices]\n",
    "#         nearest_distances = [dist for _, dist in nearest_neighbors]\n",
    "\n",
    "#         return nearest_indices, nearest_points, nearest_distances\n",
    "\n",
    "#     def find_k(self, query, bucket_indices, K):\n",
    "#         distances = []\n",
    "#         for index in bucket_indices:\n",
    "#             point = self.dataset[index]\n",
    "#             dist = distance(query, point) \n",
    "#             distances.append((index, dist))\n",
    "\n",
    "#         return sorted(distances, key=lambda x: x[1])[:K] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    def __init__(self, dataset, num_hashes=5):\n",
    "        self.dataset = dataset\n",
    "        self.num_hashes = num_hashes\n",
    "        self.dataset_augmented = np.hstack((self.dataset, np.ones((self.dataset.shape[0], 1))))  # Adding bias term\n",
    "        self.hashes = self._generate_hashes()  # Generate hash functions\n",
    "        self.hash_buckets = self._create_hash_buckets()  # Create and store hash buckets (Training)\n",
    "\n",
    "    def _generate_hashes(self):\n",
    "        return np.random.randn(self.num_hashes, self.dataset.shape[1] + 1)\n",
    "\n",
    "    def _hash_(self, point):\n",
    "        point_augmented = np.append(point, 1)  \n",
    "        return np.sign(np.dot(self.hashes, point_augmented))\n",
    "\n",
    "    def _create_hash_buckets(self):\n",
    "        # Create hash buckets by hashing each point in the dataset (part of training)\n",
    "        hash_buckets = {}\n",
    "        for index, point in enumerate(self.dataset):\n",
    "            point_hash = tuple(self._hash_(point))  # Hash each point and store in a tuple\n",
    "            if point_hash not in hash_buckets:\n",
    "                hash_buckets[point_hash] = []\n",
    "            hash_buckets[point_hash].append(index)\n",
    "        return hash_buckets  # Return the dictionary of hash buckets\n",
    "\n",
    "    def query(self, query, K):\n",
    "        # In the query, use the precomputed hash buckets\n",
    "        query_hash = tuple(self._hash_(query))\n",
    "        \n",
    "        # Step 1: Look in the same bucket\n",
    "        nearest_neighbors = self.find_k(query, self.hash_buckets.get(query_hash, []), K)\n",
    "\n",
    "        # Step 2: If not enough neighbors, look in neighboring buckets by flipping bits\n",
    "        if len(nearest_neighbors) < K:\n",
    "            for i in range(self.num_hashes):\n",
    "                if len(nearest_neighbors) >= K:\n",
    "                    break\n",
    "                modified_query_hash = list(query_hash)\n",
    "                modified_query_hash[i] = 1 - modified_query_hash[i]  # Flip the i-th hash bit\n",
    "                modified_query_hash_tuple = tuple(modified_query_hash)\n",
    "                \n",
    "                if modified_query_hash_tuple in self.hash_buckets:\n",
    "                    remaining_neighbors = self.find_k(query, self.hash_buckets[modified_query_hash_tuple], K - len(nearest_neighbors))\n",
    "                    nearest_neighbors.extend(remaining_neighbors)\n",
    "\n",
    "        # Sort to ensure top K nearest neighbors\n",
    "        nearest_neighbors = sorted(nearest_neighbors, key=lambda x: x[1])[:K]\n",
    "        nearest_indices = [index for index, _ in nearest_neighbors]\n",
    "        nearest_points = self.dataset[nearest_indices]\n",
    "        nearest_distances = [dist for _, dist in nearest_neighbors]\n",
    "\n",
    "        return nearest_indices, nearest_points, nearest_distances\n",
    "\n",
    "    def find_k(self, query, bucket_indices, K):\n",
    "        distances = []\n",
    "        for index in bucket_indices:\n",
    "            point = self.dataset[index]\n",
    "            dist = distance(query, point)\n",
    "            distances.append((index, dist))\n",
    "        return sorted(distances, key=lambda x: x[1])[:K]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Point: [0.88246101 0.0637186  0.3207994  0.88341957 0.79928569]\n",
      "[371, 919, 241, 289, 155, 243, 834, 411]\n",
      "[[0.86951383 0.09239464 0.38818843 0.91664639 0.92468818]\n",
      " [0.73215984 0.17619877 0.59477978 0.80594397 0.91370886]\n",
      " [0.98597543 0.26231166 0.65130941 0.82410741 0.88693687]\n",
      " [0.93711472 0.00645282 0.69751127 0.54277892 0.94540154]\n",
      " [0.98497918 0.23442819 0.37180185 0.37815929 0.89794973]\n",
      " [0.66388381 0.22948614 0.55872825 0.43446548 0.90508384]\n",
      " [0.85919142 0.65504589 0.77263651 0.50429122 0.96661007]\n",
      " [0.99276238 0.62000479 0.71067845 0.3977653  0.98017266]]\n",
      "[0.1495360602159034, 0.3597257475076693, 0.41302780898146346, 0.5343820052080215, 0.5543243934141284, 0.5870417679245818, 0.8521161715656228, 0.8615141097391049]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_hashes = 15 \n",
    "\n",
    "lsh = LSH(ds, num_hashes)  ## Training \n",
    "nearest_indices, nearest_points, nearest_distances = lsh.query(q, K)  # Testing\n",
    "\n",
    "print(\"Query Point:\", q)\n",
    "print(nearest_indices)\n",
    "print(nearest_points)\n",
    "print(nearest_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 0.07346391677856445 seconds\n",
      "Training Memory Usage: 0.1219949722290039 MB\n",
      "Testing Time: 0.0 seconds\n",
      "Testing Memory Usage: 0.0018177032470703125 MB\n",
      "Query Point: [0.88246101 0.0637186  0.3207994  0.88341957 0.79928569]\n",
      "Indices of Nearest Neighbors: [371, 59, 636, 632, 35, 864, 266, 269, 760, 805]\n",
      "Nearest Points: [[0.86951383 0.09239464 0.38818843 0.91664639 0.92468818]\n",
      " [0.85552405 0.01579039 0.33846085 0.83898189 0.61377044]\n",
      " [0.76167036 0.2609945  0.21365706 0.93584771 0.73211638]\n",
      " [0.77120675 0.06450603 0.40366192 0.80754863 0.56411609]\n",
      " [0.9413087  0.08627977 0.07070335 0.99889504 0.8461975 ]\n",
      " [0.93387277 0.02893653 0.10424307 0.91198749 0.60313695]\n",
      " [0.74082376 0.16473439 0.18874916 0.78536955 0.55504274]\n",
      " [0.60350425 0.14676321 0.45451395 0.98776797 0.60699443]\n",
      " [0.66281775 0.03819977 0.3887972  0.78144778 0.48748142]\n",
      " [0.67662317 0.44521113 0.37882831 0.97385696 0.87558704]]\n",
      "Distances to Nearest Points: [0.1495360602159034, 0.1993119526203916, 0.2687900081947762, 0.28338219290520195, 0.2864531235586774, 0.3000667501635856, 0.3420102236667438, 0.387888357042807, 0.4014202564686281, 0.45307167091311085]\n"
     ]
    }
   ],
   "source": [
    "#______________________________Remove after seeing_____________________________\n",
    "\n",
    "# Number of hash functions\n",
    "num_hashes = 15 \n",
    "\n",
    "# Measure training time and memory usage\n",
    "tracemalloc.start()\n",
    "start_training_time = time.time()\n",
    "lsh = LSH(ds, num_hashes)  # Training phase: initializing LSH and generating hashes\n",
    "end_training_time = time.time()\n",
    "training_memory, _ = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "training_time = end_training_time - start_training_time\n",
    "\n",
    "# Measure testing time and memory usage\n",
    "tracemalloc.start()\n",
    "start_testing_time = time.time()\n",
    "nearest_indices, nearest_points, nearest_distances = lsh.query(q, K)  # Testing phase: querying\n",
    "end_testing_time = time.time()\n",
    "testing_memory, _ = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "testing_time = end_testing_time - start_testing_time\n",
    "\n",
    "# Convert memory usage to MB\n",
    "training_memory_mb = training_memory / (1024 * 1024)\n",
    "testing_memory_mb = testing_memory / (1024 * 1024)\n",
    "\n",
    "# Output results\n",
    "print(\"Training Time:\", training_time, \"seconds\")\n",
    "print(\"Training Memory Usage:\", training_memory_mb, \"MB\")\n",
    "print(\"Testing Time:\", testing_time, \"seconds\")\n",
    "print(\"Testing Memory Usage:\", testing_memory_mb, \"MB\")\n",
    "print(\"Query Point:\", q)\n",
    "print(\"Indices of Nearest Neighbors:\", nearest_indices)\n",
    "print(\"Nearest Points:\", nearest_points)\n",
    "print(\"Distances to Nearest Points:\", nearest_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Time: 0.04881477355957031 seconds\n",
      "Testing Memory Usage: 0.010530471801757812 MB\n",
      "Indices of Nearest Neighbors: [371  59 636 632  35 864 720 266 681 958]\n",
      "Nearest Points: [[0.86951383 0.09239464 0.38818843 0.91664639 0.92468818]\n",
      " [0.85552405 0.01579039 0.33846085 0.83898189 0.61377044]\n",
      " [0.76167036 0.2609945  0.21365706 0.93584771 0.73211638]\n",
      " [0.77120675 0.06450603 0.40366192 0.80754863 0.56411609]\n",
      " [0.9413087  0.08627977 0.07070335 0.99889504 0.8461975 ]\n",
      " [0.93387277 0.02893653 0.10424307 0.91198749 0.60313695]\n",
      " [0.62704784 0.04864034 0.49964467 0.92428542 0.83798771]\n",
      " [0.74082376 0.16473439 0.18874916 0.78536955 0.55504274]\n",
      " [0.61781287 0.04742206 0.09300555 0.93059198 0.74878784]\n",
      " [0.97369146 0.0387304  0.46513792 0.7260619  0.52914708]]\n",
      "Distances to Nearest Points: [0.14953606 0.19931195 0.26879001 0.28338219 0.28645312 0.30006675\n",
      " 0.31720142 0.34201022 0.35632783 0.35709567]\n"
     ]
    }
   ],
   "source": [
    "#______________________________________Remove after seeing__________________________\n",
    "tracemalloc.start()  # Start tracking memory\n",
    "\n",
    "start_test_time = time.time()\n",
    "indices, points, distances = naive_knn(q, ds, K)\n",
    "end_test_time = time.time()\n",
    "\n",
    "current_memory, _ = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "# Calculate testing time and memory usage\n",
    "testing_time = end_test_time - start_test_time\n",
    "testing_memory_mb = current_memory / (1024 * 1024)  # Convert bytes to MB\n",
    "\n",
    "# Output results\n",
    "print(\"Testing Time:\", testing_time, \"seconds\")\n",
    "print(\"Testing Memory Usage:\", testing_memory_mb, \"MB\")\n",
    "print(\"Indices of Nearest Neighbors:\", indices)\n",
    "print(\"Nearest Points:\", points)\n",
    "print(\"Distances to Nearest Points:\", distances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
